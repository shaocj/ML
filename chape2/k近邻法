  k近邻法是一种基本分类与回归方法。实际上利用训练数据集对特征向量空间进行划分。三大基本要素：k值的选择、距离度量及分类决策规则。
定义：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。
  k值选择的影响：
较小的k值，近似误差减小，估计误差变大，意味着整体模型变得复杂，容易发生过拟合。较大的k值，近似误差变大，估计误差变小，
  kd树：
kd树是二叉树，表示对k维空间的一个划分，构建KD树相当于不断地用垂直于坐标轴的超平面将K维空间进行划分。
构造方法：构造根节点，使根节点对应于K维空间中包含所有实例点的超矩形区域。在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点
，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，分为左右两个区域，这个过程知道没有实例终止。
  搜索kd树：
目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部。
1、在kd 树种找出包含目标点x的叶节点：从根节点出发，递归往下访问kd树。目标点x坐标小于切分点，移到左子节点，
直到子节点为叶节点为止。
2、以此叶节点为当前最近点
3、递归向上回退，在每个节点进行以下操作：
如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为当前最近点；当前UI进店一定存在于该节点一个子节点对应的区域。检查该子节点的
父节点的另一子节点对应的区域是否有更近点。
4、当回退到根节点时，搜索结束，最后的当前最近点即为x的最近邻点。


k-近邻算法必须保存全部数据集，如果训练数据集很大，不许使用大量存储空间。由于对数据集中的每个数据进行计算距离值，可能非常耗时。
另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本有什么特征。
