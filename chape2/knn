3.1、k值设定为多大？
k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。
（对距离加权，可以降低k值设定的影响）
k值通常是采用交叉检验来确定（以k=1为基准）
经验规则：k一般低于训练样本数的平方根

3.2、类别如何判定最合适？
投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。而具体如何加权，需要根据具体的业务和数据特性来探索

3.3、如何选择合适的距离衡量？
高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。
变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。

3.4、训练样本是否要一视同仁？
在训练集中，有些样本可能是更值得依赖的。
也可以说是样本数据质量的问题
可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。

3.5、性能问题？
kNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找k个近邻）。
懒惰的后果：构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并计算距离。
已经有一些方法提高计算的效率，例如压缩训练样本量 等。

1.4 算法不足之处
1.	样本不平衡容易导致结果错误
	如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。
	改善方法：对此可以采用权值的方法（和该样本距离小的邻居权值大）来改进。

2.	计算量较大
	因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。
	改善方法：事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。
该方法比较适用于样本容量比较大的类域的分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。
