来源：10天
    从数学层面理解，PCA 的目标就是在高维数据中找到最大方差（离散程度）的方向，并将数据映射到一个维度不大于原始数据的新的子空间上。方差比较大，比较分散，
信息不确定性就打，信息含量就多。
    第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴选择和第一个坐标轴正交且具有方差次大的方向。此过程一直重复，重复次数为原始数据中特征的
数目。大部分方差都集中在最前面的几个新坐标轴中。因此，可以忽略余下的坐标轴，即对数据进行了降维处理。
    将n 维数据降为r 维：
    选r 个方向（基向量）,其中r<n，让数据集分别投影到这r 个方向上，投影后的各个方向尽量分散。
    X,Y 可以看做是多维数据中抽取的其中两维特征。协方差越大，相关性越大。当协方差为0，表示两个特征向量完全独立，为了更多表示信息量，希望两个特征之间不
相关（即独立）。
    PCA 总结:
    降维/压缩问题则是选择数据具有代表性的特征，在保持数据多样性的基础;上，规避掉大量的特征冗余和噪声，不过这个过程也很有可能会损失一些有用的
模式信息。经过大量的实践证明，相比较损失的少部分模型性能，维度压缩能够节省大量用于模型训练的时间，这样一来PCA 所带来的模型综合效率变得更为
划算。
https://www.cnblogs.com/tbiiann/p/6259459.html
http://blog.codinglabs.org/articles/pca-tutorial.html
https://blog.csdn.net/lanchunhui/article/details/53996070
    我们可以认为两者是线性相关的，即知道下单数，我们可以得到付款数，这里很明显这两个属性维度有冗余，去掉下单数，保留付款数，
明显能再保证原有数据分布和信息的情况下有效简化数据，对于后面的模型学习会缩短不少时间和空间开销。这就是降维，当然并不是
所有数据中都会有过于明显线性相关的属性维度，我们降维后最终的目标是各个属性维度之间线性无关。
*****PCA降维步骤原理******

首先既然要度量那些是否存在相关的属性，我们就要用到协方差，在博客相关分析中有介绍，这里不再赘述，协方差衡量的是2维属性
间的相关性，对于n个维度的属性，就需要协方差矩阵，其对角线为各维度的方差。

步骤：


                       设有m条n维数据。

                      1）将原始数据按列组成n行m列矩阵X

                      2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

                      3）求出协方差矩阵

                      4）求出协方差矩阵的特征值及对应的特征向量r

                      5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P

                      6）即为降维到k维后的数据

LDA：
    线性判别分析（Linear Discriminant Analysis，LDA）是一种可作为特征抽取的技术，可以提高数据分析过程中的计算效率，同时对于不适用于正则化的模型，
它可以降低模型灾难带来的过拟合。
LDA 的概念与PCA 区别与联系
（1）PCA 试图在数据集中找到方差最大的正交主成分量的轴，而LDA 的目标是发现可以最优化分类的特征子空间。LDA 和PCA 都是可以降低数据集维度的线性转化技巧。
（2）PCA 是一种无监督算法（不需要类标签的参与），而LDA 是监督算法。
（3）在图像识别某些情况下，如每个类别中只有少量样本使用PCA 最为预处理工具的分类结果更佳。
LDA 算法步骤：
    前提假设：（1）数据呈现正态分布（2）数据具有相同的协方差矩阵（3）数据特征相互独立的，一定注意：这里如果上述一个或三个假设都不成立，LDA仍旧可以很
好的完成降维任务。
注：看10天
