来源：10天
    从数学层面理解，PCA 的目标就是在高维数据中找到最大方差（离散程度）的方向，并将数据映射到一个维度不大于原始数据的新的子空间上。方差比较大，比较分散，
信息不确定性就打，信息含量就多。
    第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴选择和第一个坐标轴正交且具有方差次大的方向。此过程一直重复，重复次数为原始数据中特征的
数目。大部分方差都集中在最前面的几个新坐标轴中。因此，可以忽略余下的坐标轴，即对数据进行了降维处理。
    将n 维数据降为r 维：
    选r 个方向（基向量）,其中r<n，让数据集分别投影到这r 个方向上，投影后的各个方向尽量分散。
    X,Y 可以看做是多维数据中抽取的其中两维特征。协方差越大，相关性越大。当协方差为0，表示两个特征向量完全独立，为了更多表示信息量，希望两个特征之间不
相关（即独立）。
    PCA 总结:
    降维/压缩问题则是选择数据具有代表性的特征，在保持数据多样性的基础;上，规避掉大量的特征冗余和噪声，不过这个过程也很有可能会损失一些有用的
模式信息。经过大量的实践证明，相比较损失的少部分模型性能，维度压缩能够节省大量用于模型训练的时间，这样一来PCA 所带来的模型综合效率变得更为
划算。
https://www.cnblogs.com/tbiiann/p/6259459.html
http://blog.codinglabs.org/articles/pca-tutorial.html
https://blog.csdn.net/lanchunhui/article/details/53996070
    我们可以认为两者是线性相关的，即知道下单数，我们可以得到付款数，这里很明显这两个属性维度有冗余，去掉下单数，保留付款数，
明显能再保证原有数据分布和信息的情况下有效简化数据，对于后面的模型学习会缩短不少时间和空间开销。这就是降维，当然并不是
所有数据中都会有过于明显线性相关的属性维度，我们降维后最终的目标是各个属性维度之间线性无关。
*****PCA降维步骤原理******

首先既然要度量那些是否存在相关的属性，我们就要用到协方差，在博客相关分析中有介绍，这里不再赘述，协方差衡量的是2维属性
间的相关性，对于n个维度的属性，就需要协方差矩阵，其对角线为各维度的方差。

步骤：


                       设有m条n维数据。

                      1）将原始数据按列组成n行m列矩阵X

                      2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

                      3）求出协方差矩阵

                      4）求出协方差矩阵的特征值及对应的特征向量r

                      5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P

                      6）即为降维到k维后的数据

LDA：
    线性判别分析（Linear Discriminant Analysis，LDA）是一种可作为特征抽取的技术，可以提高数据分析过程中的计算效率，同时对于不适用于正则化的模型，
它可以降低模型灾难带来的过拟合。
LDA 的概念与PCA 区别与联系
（1）PCA 试图在数据集中找到方差最大的正交主成分量的轴，而LDA 的目标是发现可以最优化分类的特征子空间。LDA 和PCA 都是可以降低数据集维度的线性转化技巧。
（2）PCA 是一种无监督算法（不需要类标签的参与），而LDA 是监督算法。
（3）在图像识别某些情况下，如每个类别中只有少量样本使用PCA 最为预处理工具的分类结果更佳。
LDA 算法步骤：
    前提假设：（1）数据呈现正态分布（2）数据具有相同的协方差矩阵（3）数据特征相互独立的，一定注意：这里如果上述一个或三个假设都不成立，LDA仍旧可以很
好的完成降维任务。
注：看10天
  LDA的基本思想是: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离，在对新样本进行分类时，
将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。
LDA和PCA对比

    LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。

　　　首先我们看看相同点：

　　　　1）两者均可以对数据进行降维。

　　　　2）两者在降维时均使用了矩阵特征分解的思想。（求特征值、特征向量）

　　　　3）两者都假设数据符合高斯分布。

　　　我们接着看看不同点：

　　　　1）LDA是有监督的降维方法，而PCA是无监督的降维方法

　　　　2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。

　　　　3）LDA除了可以用于降维，还可以用于分类。

　　　　4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。
 LDA算法的主要优点有：

　　　　1）在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。

　　　　2）LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。

　 LDA算法的主要缺点有：

　　　　1）LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。

　　　　2）LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。

　　　　3）LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。

　　　　4）LDA可能过度拟合数据。
https://blog.csdn.net/u012679707/article/details/80529252
http://www.cnblogs.com/pinard/p/6244265.html

SVD奇异值分解：（看10天）
可以发现，求特征值必须要求矩阵是方阵，而求奇异值对任意矩阵都可以。。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。
选取奇异值个数k 策略：
（1）保留矩阵中90%（或其他比例）的能量信息；
总能量：所有奇异值求平方和
（2）有上万个奇异值时，保留前面的2000 或3000 个(不优雅)。
足够了解数据做出的假设
