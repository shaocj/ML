Adaboost优缺点：
优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数可调整。
缺点：对离群数据点敏感。
适用数据类型：数值型和标称型数据。

决策树优缺点：
（1） 决策树的优点：
 直观，便于理解，小规模数据集有效
 执行效率高，执行只需要一次构建，可反复使用
（2）决策树的缺点：
 处理连续变量不好，较难预测连续字段
 类别较多时，错误增加的比较快
 对于时间序列数据需要做很多的预处理
 可规模性一般
 实际分类的时候只能根据一个字段进行

随机森林优缺点：
优点：
	实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；
	相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
	能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
	对于不平衡的数据集，可以平衡误差；
	相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；
	训练完成后可以给出哪些特征比较重要。
缺点：
	在噪声过大的分类和回归问题还是容易过拟合；
	相比于单一决策树，它的随机性让我们难以对模型进行解释。

GBDT优缺点：
优点：
预测精度高
适合低维数据
能处理非线性数据
可以灵活处理各种类型的数据，包括连续值和离散值。
在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。
使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。
缺点：
由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。
如果数据维度较高时会加大算法的计算复杂度

随机森林和GBDT的区别：
	1、随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
	2、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
	3、组成随机森林的树可以并行生成；而GBDT只能是串行生成。
	4、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
	5、随机森林对异常值不敏感；GBDT对异常值非常敏感。
	6、随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
	7、随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。
	--------------------- 


逻辑斯特回归优缺点：
优点：
•	形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
•	模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
•	训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
•	资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
•	方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。
 缺点:
•	准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
•	很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
•	处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
•	逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。



