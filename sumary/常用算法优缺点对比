bagging和boosting
先介绍Bagging方法：
Bagging即套袋法，其算法过程如下：
1.	从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
2.	每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
3.	对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
Boosting：
在分类问题中它通过改变训练样本的权重，学习多个分类器，并将分类器进行线性组合。
AdaBoosting方式每次使用的是全部的样本，每轮训练改变样本的权重。下一轮训练的目标是找到一个函数f 来拟合上一轮的残差。当残差足够小或者达到设置的最大迭代次数则停止。Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重。（对的残差小，错的残差大）梯度提升的Boosting方式是使用代价函数对上一轮训练出的模型函数f的偏导来拟合残差。
Bagging，Boosting二者之间的区别
Bagging和Boosting的区别：
1）样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2）样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
3）预测函数权重：
Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4）是否可以并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
5）为什么说bagging是减少variance，而boosting是减少bias？

Adaboost优缺点：
Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。是一种典型的boosting算法。下面是总结下它的优缺点。

优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数可调整。adaboost是一种有很高精度的分类器。可以使用各种方法构建子分类器，Adaboost算法提供的是框架。当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。简单，不用做特征筛选。不容易发生overfitting。
缺点：对离群数据点敏感。
适用数据类型：数值型和标称型数据。

决策树优缺点：
（1） 决策树的优点：
 直观，便于理解，小规模数据集有效
 执行效率高，执行只需要一次构建，可反复使用
（2）决策树的缺点：
 处理连续变量不好，较难预测连续字段
 类别较多时，错误增加的比较快
 对于时间序列数据需要做很多的预处理
 可规模性一般
 实际分类的时候只能根据一个字段进行

随机森林优缺点：
优点：
	实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；
	相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
	能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
	对于不平衡的数据集，可以平衡误差；
	相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；
	训练完成后可以给出哪些特征比较重要。
缺点：
	在噪声过大的分类和回归问题还是容易过拟合；
	相比于单一决策树，它的随机性让我们难以对模型进行解释。

GBDT优缺点：
优点：
预测精度高
适合低维数据
能处理非线性数据
可以灵活处理各种类型的数据，包括连续值和离散值。
在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。
使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。
缺点：
由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。
如果数据维度较高时会加大算法的计算复杂度

随机森林和GBDT的区别：
	1、随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
	2、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
	3、组成随机森林的树可以并行生成；而GBDT只能是串行生成。
	4、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
	5、随机森林对异常值不敏感；GBDT对异常值非常敏感。
	6、随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
	7、随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。
	--------------------- 


逻辑斯特回归优缺点：
优点：
•	形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
•	模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
•	训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
•	资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
•	方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。
 缺点:
•	准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
•	很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
•	处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
•	逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。
K-Means聚类
优点
算法简单，容易实现 ；
对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。
算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。
缺点
对数据类型要求较高，适合数值型数据；
可能收敛到局部最小值，在大规模数据上收敛较慢
K值比较难以选取；
对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；
不适合于发现非凸面形状的簇，或者大小差别很大的簇。
对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。

SVM支持向量机:
高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。
优点
可以解决高维问题，即大型特征空间；
能够处理非线性特征的相互作用；
无需依赖整个数据；
可以提高泛化能力；
需要对数据提前归一化，很多人使用的时候忽略了这一点，毕竟是基于距离的模型，所以LR也需要归一化

缺点
当观测样本很多时，效率并不是很高；
一个可行的解决办法是模仿随机森林，对数据分解，训练多个模型，然后求平均，时间复杂度降低p倍，分多少份，降多少倍
对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；
对缺失数据敏感；
对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：
第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；
第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；
第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。
对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。
朴素贝叶斯
朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布），非常简单，你只是做了一堆计数。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。
优点：
朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。
对小规模的数据表现很好，能个处理多分类任务，适合增量式训练；
对缺失数据不太敏感，算法也比较简单，常用于文本分类。
缺点：
需要计算先验概率；
分类决策存在错误率；
对输入数据的表达形式很敏感。

