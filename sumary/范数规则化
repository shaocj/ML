
https://blog.csdn.net/zouxy09/article/details/24971995
    任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。
个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大
家才把目光和万千宠爱转于L1范数。
  condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生
多大的变化。也就是系统对微小变化的敏感度。也就是系统对微小变化的敏感度。condition number值小的就是well-conditioned的，
大的就是ill-conditioned的。
    L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，
但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易
产生过拟合现象。为什么越小的参数说明模型越简单？我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响
很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。
     对condition number来个一句话总结：conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，
如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果
一个系统是ill-conditioned的，它的输出结果就不要太相信了。
    L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。
    这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。

       因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。
 Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。
